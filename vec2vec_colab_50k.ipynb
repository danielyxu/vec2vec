{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vec2Vec: Colab Reproduction (~50k Examples)\n",
        "\n",
        "This notebook reproduces the main vec2vec experiment from the paper:\n",
        "**\"Harnessing the Universal Geometry of Embeddings\"** (Jha et al., 2025)\n",
        "\n",
        "## Experiment Details\n",
        "- **Source Model**: Stella (`stella` - unsupervised embedding model)\n",
        "- **Target Model**: GTE (`gte` - supervised embedding model)\n",
        "- **Dataset**: Natural Questions (NQ)\n",
        "- **Training Size**: ~50k examples (25k per encoder)\n",
        "- **Architecture**: ResNet MLP with adapters + adversarial training\n",
        "- **Losses**: Reconstruction + VSP + Cross-chain translation/VSP + GAN losses\n",
        "\n",
        "This is a scaled-down version of the full experiment designed to run on a single Colab GPU (T4/L4) in a few hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check GPU and environment\n",
        "!nvidia-smi\n",
        "\n",
        "import sys\n",
        "print(f\"\\nPython version: {sys.version}\")\n",
        "\n",
        "# Check CUDA availability\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "except ImportError:\n",
        "    print(\"PyTorch will be installed in the next step\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clone the vec2vec repository\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/rjha18/supervised_disc.git\"  # Official vec2vec repo\n",
        "REPO_DIR = \"/content/vec2vec\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"Repository already exists at {REPO_DIR}\")\n",
        "    %cd {REPO_DIR}\n",
        "    !git pull origin main\n",
        "else:\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "    %cd {REPO_DIR}\n",
        "\n",
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "# Core packages for vec2vec\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers>=4.29.0 sentence-transformers>=2.2.0\n",
        "!pip install -q datasets>=2.12.0 huggingface_hub>=0.15.0\n",
        "!pip install -q accelerate>=0.20.0\n",
        "!pip install -q wandb\n",
        "!pip install -q scikit-learn scipy matplotlib\n",
        "!pip install -q toml\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "import transformers\n",
        "import sentence_transformers\n",
        "import accelerate\n",
        "\n",
        "print(f\"\\nInstalled versions:\")\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  Transformers: {transformers.__version__}\")\n",
        "print(f\"  Sentence-Transformers: {sentence_transformers.__version__}\")\n",
        "print(f\"  Accelerate: {accelerate.__version__}\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data & Embedding Preparation\n",
        "\n",
        "Vec2vec uses the **Natural Questions (NQ)** dataset from the BeIR benchmark. The embeddings are generated on-the-fly during training using the source (Stella) and target (GTE) embedding models.\n",
        "\n",
        "The data loading pipeline:\n",
        "1. Loads NQ corpus from HuggingFace datasets\n",
        "2. Splits into train/validation sets\n",
        "3. Creates tokenized batches for both encoders\n",
        "4. Generates embeddings during forward pass\n",
        "\n",
        "We'll use **25,000 samples per encoder** (50k total), which is sufficient to demonstrate the approach while keeping training time reasonable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test data loading to ensure everything is properly set up\n",
        "import sys\n",
        "sys.path.insert(0, '/content/vec2vec')\n",
        "\n",
        "from utils.streaming_utils import load_streaming_embeddings\n",
        "\n",
        "# Load the NQ dataset\n",
        "print(\"Loading NQ dataset...\")\n",
        "dset = load_streaming_embeddings(\"nq\")\n",
        "print(f\"Dataset loaded: {len(dset)} examples\")\n",
        "print(f\"\\nSample entry keys: {list(dset[0].keys())}\")\n",
        "\n",
        "# Show a sample\n",
        "sample = dset[0]\n",
        "text_key = 'text' if 'text' in sample else list(sample.keys())[0]\n",
        "print(f\"\\nSample text (truncated): {sample[text_key][:200]}...\")\n",
        "\n",
        "# Confirm we have enough data\n",
        "TRAIN_SIZE = 25000  # per encoder\n",
        "VAL_SIZE = 4096\n",
        "REQUIRED = TRAIN_SIZE * 2 + VAL_SIZE\n",
        "\n",
        "if len(dset) >= REQUIRED:\n",
        "    print(f\"\\n✓ Dataset has {len(dset)} examples (need {REQUIRED} for this run)\")\n",
        "else:\n",
        "    print(f\"\\n⚠ Dataset has {len(dset)} examples, adjusting train size...\")\n",
        "    TRAIN_SIZE = (len(dset) - VAL_SIZE) // 2\n",
        "    print(f\"  New train size: {TRAIN_SIZE} per encoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "We'll use the `unsupervised.toml` config as the base, which is the main experiment config from the paper.\n",
        "\n",
        "### Key Hyperparameters for Colab Run:\n",
        "- **num_points**: 25,000 (samples per encoder, 50k total)\n",
        "- **epochs**: 10 (reduced from 80 for faster iteration)\n",
        "- **batch_size**: 128 (reduced from 256 for GPU memory)\n",
        "- **learning_rate**: 2e-5 (default)\n",
        "- **mixed_precision**: fp16 (for memory efficiency)\n",
        "\n",
        "### Architecture:\n",
        "- **Translator**: ResNet MLP with adapters (`style='res_mlp'`)\n",
        "- **Adapter dimension**: 1024\n",
        "- **Discriminator**: 5-layer MLP with residuals\n",
        "- **GAN type**: Least squares GAN\n",
        "\n",
        "### Losses (with coefficients):\n",
        "- Reconstruction: 1.0\n",
        "- VSP (Vector Space Preservation): 1.0\n",
        "- Cross-chain translation: 10.0\n",
        "- Cross-chain VSP: 10.0\n",
        "- Adversarial (generator): 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display and modify configuration for Colab run\n",
        "import toml\n",
        "import os\n",
        "\n",
        "# Load base config\n",
        "config_path = '/content/vec2vec/configs/unsupervised.toml'\n",
        "with open(config_path, 'r') as f:\n",
        "    config = toml.load(f)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASE CONFIGURATION (unsupervised.toml)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Key settings\n",
        "print(f\"\\n[General]\")\n",
        "print(f\"  Dataset: {config['general']['dataset']}\")\n",
        "print(f\"  Unsup Model: {config['general']['unsup_emb']}\")\n",
        "print(f\"  Sup Model: {config['general']['sup_emb']}\")\n",
        "print(f\"  Mixed Precision: {config['general']['mixed_precision']}\")\n",
        "\n",
        "print(f\"\\n[Translator]\")\n",
        "print(f\"  Style: {config['translator']['style']}\")\n",
        "print(f\"  Adapter Dim: {config['translator']['d_adapter']}\")\n",
        "print(f\"  Depth: {config['translator']['depth']}\")\n",
        "\n",
        "print(f\"\\n[Training]\")\n",
        "print(f\"  Batch Size: {config['train']['bs']}\")\n",
        "print(f\"  Learning Rate: {config['train']['lr']}\")\n",
        "\n",
        "print(f\"\\n[Losses]\")\n",
        "print(f\"  Reconstruction: {config['train']['loss_coefficient_rec']}\")\n",
        "print(f\"  VSP: {config['train']['loss_coefficient_vsp']}\")\n",
        "print(f\"  CC Trans: {config['train']['loss_coefficient_cc_trans']}\")\n",
        "print(f\"  CC VSP: {config['train']['loss_coefficient_cc_vsp']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COLAB OVERRIDES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define Colab-specific settings\n",
        "COLAB_CONFIG = {\n",
        "    'num_points': 25000,       # 25k per encoder (50k total)\n",
        "    'epochs': 10,              # Reduced for Colab\n",
        "    'bs': 128,                 # Reduced batch size for T4\n",
        "    'val_size': 4096,          # Validation set size\n",
        "    'use_wandb': False,        # Disable W&B for simplicity\n",
        "    'min_epochs': 5,           # Lower minimum epochs\n",
        "    'patience': 5,             # Earlier stopping\n",
        "}\n",
        "\n",
        "for k, v in COLAB_CONFIG.items():\n",
        "    print(f\"  --{k} {v}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Launch training\n",
        "import os\n",
        "os.chdir('/content/vec2vec')\n",
        "\n",
        "# Create output directory\n",
        "OUTPUT_DIR = '/content/vec2vec/outputs/colab_50k'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Build training command\n",
        "CMD = f\"\"\"\n",
        "python train.py unsupervised \\\\\n",
        "    --num_points 25000 \\\\\n",
        "    --epochs 10 \\\\\n",
        "    --bs 128 \\\\\n",
        "    --val_size 4096 \\\\\n",
        "    --use_wandb false \\\\\n",
        "    --min_epochs 5 \\\\\n",
        "    --patience 5 \\\\\n",
        "    --save_dir '{OUTPUT_DIR}/{{}}'\n",
        "\"\"\"\n",
        "\n",
        "print(\"Training command:\")\n",
        "print(CMD)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting training... (this may take 1-3 hours on a T4)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Run training\n",
        "!{CMD}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "After training, we evaluate the translator on held-out test data using the following metrics:\n",
        "\n",
        "1. **Cosine Similarity**: Average cosine similarity between translated embeddings and true target embeddings\n",
        "2. **Top-1 Accuracy**: Percentage of samples where the translated embedding's nearest neighbor is the correct target\n",
        "3. **Mean Rank**: Average rank of the correct target among all candidates (lower is better)\n",
        "4. **VSP (Vector Space Preservation)**: How well pairwise similarities are preserved after translation\n",
        "\n",
        "The evaluation uses the repo's built-in `eval.py` script which loads the trained checkpoint and computes all metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find the checkpoint directory\n",
        "import os\n",
        "import glob\n",
        "\n",
        "OUTPUT_DIR = '/content/vec2vec/outputs/colab_50k'\n",
        "\n",
        "# Find the most recent checkpoint\n",
        "checkpoint_dirs = glob.glob(f\"{OUTPUT_DIR}/**/model.pt\", recursive=True)\n",
        "if not checkpoint_dirs:\n",
        "    # Also check default save location\n",
        "    checkpoint_dirs = glob.glob(\"/content/vec2vec/finetuning_unsupervised/**/model.pt\", recursive=True)\n",
        "\n",
        "if checkpoint_dirs:\n",
        "    # Get the directory containing model.pt\n",
        "    checkpoint_path = os.path.dirname(sorted(checkpoint_dirs, key=os.path.getmtime)[-1])\n",
        "    print(f\"Found checkpoint at: {checkpoint_path}\")\n",
        "    \n",
        "    # Run evaluation\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Running evaluation...\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    \n",
        "    !python eval.py \"{checkpoint_path}\" --num_points 8000\n",
        "else:\n",
        "    print(\"No checkpoint found. Please run training first.\")\n",
        "    print(f\"Searched in: {OUTPUT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Custom evaluation with detailed metrics\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "sys.path.insert(0, '/content/vec2vec')\n",
        "\n",
        "OUTPUT_DIR = '/content/vec2vec/outputs/colab_50k'\n",
        "\n",
        "# Find checkpoint\n",
        "checkpoint_dirs = glob.glob(f\"{OUTPUT_DIR}/**/model.pt\", recursive=True)\n",
        "if not checkpoint_dirs:\n",
        "    checkpoint_dirs = glob.glob(\"/content/vec2vec/finetuning_unsupervised/**/model.pt\", recursive=True)\n",
        "\n",
        "if checkpoint_dirs:\n",
        "    checkpoint_path = os.path.dirname(sorted(checkpoint_dirs, key=os.path.getmtime)[-1])\n",
        "    \n",
        "    # Load config from checkpoint\n",
        "    import toml\n",
        "    config_file = os.path.join(checkpoint_path, 'config.toml')\n",
        "    if os.path.exists(config_file):\n",
        "        config = toml.load(config_file)\n",
        "        \n",
        "        # Extract key metrics from the run\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Create metrics table\n",
        "        print(\"\\n{:<15} {:<12} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "            'Model Pair', 'Train Size', 'Test Size', 'Cosine', 'Top-1', 'Rank'\n",
        "        ))\n",
        "        print(\"-\"*70)\n",
        "        \n",
        "        # These values would come from the eval.py output\n",
        "        # For now, we'll print placeholder - actual values printed by eval.py above\n",
        "        model_pair = f\"{config.get('unsup_emb', 'stella')}→{config.get('sup_emb', 'gte')}\"\n",
        "        train_size = config.get('num_points', 25000)\n",
        "        test_size = config.get('val_size', 4096)\n",
        "        \n",
        "        print(f\"{model_pair:<15} {train_size:<12} {test_size:<10} {'--':<10} {'--':<10} {'--':<10}\")\n",
        "        print(\"\\nNote: Actual metrics are printed by eval.py above.\")\n",
        "        print(\"      Look for 'trans' metrics showing translation quality.\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Check for any saved results\n",
        "        results_files = glob.glob(os.path.join(checkpoint_path, '*.json'))\n",
        "        if results_files:\n",
        "            print(\"\\nSaved result files:\")\n",
        "            for f in results_files:\n",
        "                print(f\"  - {os.path.basename(f)}\")\n",
        "    else:\n",
        "        print(f\"Config file not found at {config_file}\")\n",
        "else:\n",
        "    print(\"No checkpoint found to evaluate.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot training metrics (if available)\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "OUTPUT_DIR = '/content/vec2vec/outputs/colab_50k'\n",
        "\n",
        "# Find checkpoint\n",
        "checkpoint_dirs = glob.glob(f\"{OUTPUT_DIR}/**/model.pt\", recursive=True)\n",
        "if not checkpoint_dirs:\n",
        "    checkpoint_dirs = glob.glob(\"/content/vec2vec/finetuning_unsupervised/**/model.pt\", recursive=True)\n",
        "\n",
        "if checkpoint_dirs:\n",
        "    checkpoint_path = os.path.dirname(sorted(checkpoint_dirs, key=os.path.getmtime)[-1])\n",
        "    \n",
        "    # Look for any log files or metrics\n",
        "    log_files = glob.glob(os.path.join(checkpoint_path, '*.json'))\n",
        "    \n",
        "    # Check for wandb logs (if they were enabled)\n",
        "    wandb_logs = glob.glob(os.path.join(checkpoint_path, 'wandb', '**', '*.json'), recursive=True)\n",
        "    \n",
        "    if log_files or wandb_logs:\n",
        "        print(\"Attempting to plot training metrics...\")\n",
        "        # This would load and plot metrics if available\n",
        "        print(\"Note: Detailed per-step logging requires W&B to be enabled.\")\n",
        "    else:\n",
        "        print(\"No per-step training logs found.\")\n",
        "        print(\"\\nTo enable detailed logging:\")\n",
        "        print(\"  1. Set --use_wandb true\")\n",
        "        print(\"  2. Login with: wandb login\")\n",
        "        print(\"\\nThe heatmaps from evaluation provide visual insight into translation quality.\")\n",
        "        \n",
        "    # Check for any generated heatmaps or figures\n",
        "    figure_files = glob.glob(os.path.join(checkpoint_path, '*.png'))\n",
        "    if figure_files:\n",
        "        print(f\"\\nFound {len(figure_files)} figure(s):\")\n",
        "        for f in figure_files:\n",
        "            print(f\"  - {os.path.basename(f)}\")\n",
        "        \n",
        "        # Display the first heatmap if available\n",
        "        from IPython.display import Image, display\n",
        "        for fig_path in figure_files[:2]:  # Show first 2\n",
        "            print(f\"\\nDisplaying: {os.path.basename(fig_path)}\")\n",
        "            display(Image(filename=fig_path))\n",
        "else:\n",
        "    print(\"No checkpoint found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Next Steps\n",
        "\n",
        "### What This Notebook Reproduces\n",
        "\n",
        "This notebook trains a **vec2vec translator** to map embeddings from Stella (unsupervised) to GTE (supervised) space on the NQ dataset, using:\n",
        "\n",
        "- **50k training examples** (25k per encoder)\n",
        "- **ResNet MLP architecture** with adapters\n",
        "- **Adversarial training** with least-squares GAN\n",
        "- **Multiple losses**: reconstruction, VSP, cross-chain translation/VSP\n",
        "\n",
        "This is a scaled-down version of the full experiment (which uses 100k+ examples and 80+ epochs).\n",
        "\n",
        "---\n",
        "\n",
        "### Scaling Up\n",
        "\n",
        "To increase training scale:\n",
        "```python\n",
        "# More training data (e.g., 100k per encoder)\n",
        "--num_points 100000\n",
        "\n",
        "# More epochs\n",
        "--epochs 50\n",
        "\n",
        "# Larger batch size (if GPU memory allows)\n",
        "--bs 256\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Changing Model Pairs\n",
        "\n",
        "To translate between different embedding models:\n",
        "```python\n",
        "# GTE to GTR\n",
        "--unsup_emb gte --sup_emb gtr\n",
        "\n",
        "# E5 to GTE\n",
        "--unsup_emb e5 --sup_emb gte\n",
        "\n",
        "# Stella to SBERT\n",
        "--unsup_emb stella --sup_emb sbert\n",
        "```\n",
        "\n",
        "Available models: `gtr`, `gte`, `stella`, `e5`, `sbert`, `ember`, `gist`, `sentence-t5`, `dpr`, `jina`, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Using Different Datasets\n",
        "\n",
        "```python\n",
        "# FineWeb (larger dataset)\n",
        "--dataset fineweb\n",
        "\n",
        "# BeIR benchmark datasets\n",
        "--dataset arguana-corpus\n",
        "--dataset msmarco-corpus\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Tips\n",
        "\n",
        "1. **GAN training is unstable**: Try different seeds if training doesn't converge\n",
        "2. **Monitor validation metrics**: Early stopping uses mean rank by default\n",
        "3. **Use W&B for detailed logs**: Set `--use_wandb true` after `wandb login`\n",
        "4. **Check GPU memory**: Reduce batch size if you get OOM errors\n",
        "5. **Pre-trained weights**: Download from [GitHub releases](https://github.com/rjha18/vec2vec/releases) for full-scale trained models\n",
        "\n",
        "---\n",
        "\n",
        "### Citation\n",
        "\n",
        "```bibtex\n",
        "@misc{jha2025harnessinguniversalgeometryembeddings,\n",
        "      title={Harnessing the Universal Geometry of Embeddings}, \n",
        "      author={Rishi Jha and Collin Zhang and Vitaly Shmatikov and John X. Morris},\n",
        "      year={2025},\n",
        "      eprint={2505.12540},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.LG},\n",
        "      url={https://arxiv.org/abs/2505.12540}, \n",
        "}\n",
        "```"
      ]
    }
  ]
}