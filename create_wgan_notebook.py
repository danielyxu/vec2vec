#!/usr/bin/env python3
"""
Script to create the vec2vec Colab notebook with WGAN-GP comparison.
Generates vec2vec_colab_50k_wgan.ipynb
"""

import json

def create_notebook():
    """Create vec2vec_colab_50k_wgan.ipynb notebook with WGAN comparison."""

    cells = []

    # Cell 1: Markdown - Title and explanation
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# Vec2Vec: Colab Reproduction with WGAN Adversarial Loss (~50k Examples)\n",
            "\n",
            "This notebook reproduces and extends the main vec2vec experiment from the paper:\n",
            "**\"Harnessing the Universal Geometry of Embeddings\"** (Jha et al., 2025)\n",
            "\n",
            "## Experiment Details\n",
            "- **Source Model**: Stella (`stella` - unsupervised embedding model)\n",
            "- **Target Model**: GTE (`gte` - supervised embedding model)\n",
            "- **Dataset**: Natural Questions (NQ)\n",
            "- **Training Size**: ~50k examples (25k per encoder)\n",
            "- **Architecture**: ResNet MLP with adapters + adversarial training\n",
            "\n",
            "## Adversarial Loss Comparison\n",
            "We compare two adversarial training schemes:\n",
            "\n",
            "1. **Baseline (GAN)**: Standard GAN with least-squares loss (original vec2vec approach)\n",
            "2. **WGAN-GP**: Wasserstein GAN with gradient penalty\n",
            "   - Treats discriminators as critics with unbounded outputs\n",
            "   - Uses Wasserstein distance estimation\n",
            "   - Enforces Lipschitz constraint via gradient penalty\n",
            "\n",
            "The goal is to explore whether Wasserstein-style matching of embedding/latent distributions improves translation quality."
        ]
    })

    # Cell 2: Code - Environment check & GPU info
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Check GPU and environment\n",
            "!nvidia-smi\n",
            "\n",
            "import sys\n",
            "print(f\"\\nPython version: {sys.version}\")\n",
            "\n",
            "# Check CUDA availability\n",
            "try:\n",
            "    import torch\n",
            "    print(f\"PyTorch version: {torch.__version__}\")\n",
            "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
            "    if torch.cuda.is_available():\n",
            "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
            "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
            "except ImportError:\n",
            "    print(\"PyTorch will be installed in the next step\")"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 3: Code - Clone repo
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Clone the vec2vec repository\n",
            "import os\n",
            "\n",
            "REPO_URL = \"https://github.com/danielyxu/vec2vec.git\"  # vec2vec repo\n",
            "REPO_DIR = \"/content/vec2vec\"\n",
            "\n",
            "if os.path.exists(REPO_DIR):\n",
            "    print(f\"Repository already exists at {REPO_DIR}\")\n",
            "    %cd {REPO_DIR}\n",
            "    !git pull origin main\n",
            "else:\n",
            "    !git clone {REPO_URL} {REPO_DIR}\n",
            "    %cd {REPO_DIR}\n",
            "\n",
            "!ls -la"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 4: Code - Install dependencies
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Install dependencies\n",
            "# Core packages for vec2vec\n",
            "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
            "!pip install -q transformers>=4.29.0 sentence-transformers>=2.2.0\n",
            "!pip install -q datasets>=2.12.0 huggingface_hub>=0.15.0\n",
            "!pip install -q accelerate>=0.20.0\n",
            "!pip install -q wandb\n",
            "!pip install -q scikit-learn scipy matplotlib seaborn\n",
            "!pip install -q toml pandas\n",
            "!pip install -q nltk\n",
            "\n",
            "# Download NLTK data\n",
            "import nltk\n",
            "nltk.download('punkt', quiet=True)\n",
            "\n",
            "# Verify installation\n",
            "import torch\n",
            "import transformers\n",
            "import sentence_transformers\n",
            "import accelerate\n",
            "\n",
            "print(f\"\\nInstalled versions:\")\n",
            "print(f\"  PyTorch: {torch.__version__}\")\n",
            "print(f\"  Transformers: {transformers.__version__}\")\n",
            "print(f\"  Sentence-Transformers: {sentence_transformers.__version__}\")\n",
            "print(f\"  Accelerate: {accelerate.__version__}\")\n",
            "print(f\"  CUDA available: {torch.cuda.is_available()}\")"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 5: Markdown - Data/embedding preparation explanation
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Data & Embedding Preparation\n",
            "\n",
            "Vec2vec uses the **Natural Questions (NQ)** dataset from the BeIR benchmark. The embeddings are generated on-the-fly during training using the source (Stella) and target (GTE) embedding models.\n",
            "\n",
            "The data loading pipeline:\n",
            "1. Loads NQ corpus from HuggingFace datasets\n",
            "2. Splits into train/validation sets\n",
            "3. Creates tokenized batches for both encoders\n",
            "4. Generates embeddings during forward pass\n",
            "\n",
            "We'll use **25,000 samples per encoder** (50k total), which is sufficient to demonstrate the approach while keeping training time reasonable."
        ]
    })

    # Cell 6: Code - Data preparation
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Test data loading to ensure everything is properly set up\n",
            "import sys\n",
            "sys.path.insert(0, '/content/vec2vec')\n",
            "\n",
            "from utils.streaming_utils import load_streaming_embeddings\n",
            "\n",
            "# Load the NQ dataset\n",
            "print(\"Loading NQ dataset...\")\n",
            "dset = load_streaming_embeddings(\"nq\")\n",
            "print(f\"Dataset loaded: {len(dset)} examples\")\n",
            "print(f\"\\nSample entry keys: {list(dset[0].keys())}\")\n",
            "\n",
            "# Show a sample\n",
            "sample = dset[0]\n",
            "text_key = 'text' if 'text' in sample else list(sample.keys())[0]\n",
            "print(f\"\\nSample text (truncated): {sample[text_key][:200]}...\")\n",
            "\n",
            "# Confirm we have enough data\n",
            "TRAIN_SIZE = 25000  # per encoder\n",
            "VAL_SIZE = 4096\n",
            "REQUIRED = TRAIN_SIZE * 2 + VAL_SIZE\n",
            "\n",
            "if len(dset) >= REQUIRED:\n",
            "    print(f\"\\n Dataset has {len(dset)} examples (need {REQUIRED} for this run)\")\n",
            "else:\n",
            "    print(f\"\\n Dataset has {len(dset)} examples, adjusting train size...\")\n",
            "    TRAIN_SIZE = (len(dset) - VAL_SIZE) // 2\n",
            "    print(f\"  New train size: {TRAIN_SIZE} per encoder\")"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 7: Markdown - Adversarial loss implementation overview
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Adversarial Loss Implementation\n",
            "\n",
            "### Original vec2vec GAN Loss\n",
            "The original implementation uses **Least Squares GAN** (LSGAN) with:\n",
            "- Discriminator loss: `0.5 * (D(real)^2 + (D(fake) - 1)^2)`\n",
            "- Generator loss: `0.5 * D(fake)^2`\n",
            "\n",
            "This is applied at:\n",
            "- **Embedding level**: D1 (unsup space), D2 (sup space)\n",
            "- **Latent level**: D_latent (shared latent space)\n",
            "\n",
            "### WGAN-GP (Wasserstein GAN with Gradient Penalty)\n",
            "We implement WGAN-GP with:\n",
            "- **Critic loss**: `E[D(fake)] - E[D(real)] + lambda_gp * gradient_penalty`\n",
            "- **Generator loss**: `-E[D(fake)]`\n",
            "- **Gradient penalty**: `E[(||grad_D(x_interp)||_2 - 1)^2]`\n",
            "\n",
            "Key differences:\n",
            "1. Discriminators are treated as **critics** (no sigmoid, unbounded outputs)\n",
            "2. Uses Wasserstein distance instead of JS divergence\n",
            "3. Gradient penalty enforces 1-Lipschitz constraint\n",
            "4. Generally more stable training dynamics\n",
            "\n",
            "We add a new `gan_style=\"wgan\"` option to toggle this behavior."
        ]
    })

    # Cell 8: Code - Modify/extend adversarial loss in the codebase
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Add WGAN-GP implementation to the codebase\n",
            "import os\n",
            "\n",
            "# Read the current gan.py\n",
            "gan_path = '/content/vec2vec/utils/gan.py'\n",
            "with open(gan_path, 'r') as f:\n",
            "    gan_code = f.read()\n",
            "\n",
            "# WGAN-GP implementation to add\n",
            "wgan_code = '''\n",
            "\n",
            "class WassersteinGAN(VanillaGAN):\n",
            "    \"\"\"Wasserstein GAN with Gradient Penalty (WGAN-GP).\n",
            "    \n",
            "    Uses Wasserstein distance estimation with gradient penalty\n",
            "    to enforce Lipschitz constraint on the critic.\n",
            "    \"\"\"\n",
            "    \n",
            "    def compute_wgan_gradient_penalty(self, real_data: torch.Tensor, fake_data: torch.Tensor) -> torch.Tensor:\n",
            "        \"\"\"Compute gradient penalty for WGAN-GP.\n",
            "        \n",
            "        Interpolates between real and fake samples, computes critic output,\n",
            "        and penalizes deviation of gradient norm from 1.\n",
            "        \"\"\"\n",
            "        batch_size = real_data.size(0)\n",
            "        device = real_data.device\n",
            "        \n",
            "        # Random interpolation coefficient\n",
            "        epsilon = torch.rand(batch_size, 1, device=device)\n",
            "        \n",
            "        # Interpolate between real and fake\n",
            "        interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
            "        interpolated = interpolated.requires_grad_(True)\n",
            "        \n",
            "        # Get critic output for interpolated samples\n",
            "        d_interpolated = self.discriminator(interpolated)\n",
            "        \n",
            "        # Compute gradients\n",
            "        gradients = torch.autograd.grad(\n",
            "            outputs=d_interpolated,\n",
            "            inputs=interpolated,\n",
            "            grad_outputs=torch.ones_like(d_interpolated),\n",
            "            create_graph=True,\n",
            "            retain_graph=True,\n",
            "        )[0]\n",
            "        \n",
            "        # Compute gradient penalty: (||grad||_2 - 1)^2\n",
            "        gradients = gradients.view(batch_size, -1)\n",
            "        gradient_norm = gradients.norm(2, dim=1)\n",
            "        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
            "        \n",
            "        return gradient_penalty\n",
            "    \n",
            "    def _step_discriminator(self, real_data: torch.Tensor, fake_data: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, float, float]:\n",
            "        \"\"\"WGAN critic update step.\n",
            "        \n",
            "        Critic loss = E[D(fake)] - E[D(real)] + lambda_gp * gradient_penalty\n",
            "        \"\"\"\n",
            "        real_data = real_data.detach()\n",
            "        fake_data = fake_data.detach()\n",
            "        \n",
            "        # Critic outputs (no sigmoid - raw scores)\n",
            "        d_real = self.discriminator(real_data)\n",
            "        d_fake = self.discriminator(fake_data)\n",
            "        \n",
            "        # Wasserstein distance estimate (negative because we want to maximize)\n",
            "        # Critic wants: D(real) high, D(fake) low\n",
            "        # So critic loss = E[D(fake)] - E[D(real)]\n",
            "        wasserstein_dist = d_real.mean() - d_fake.mean()\n",
            "        critic_loss = -wasserstein_dist  # Minimize negative = maximize distance\n",
            "        \n",
            "        # Gradient penalty\n",
            "        gp_lambda = getattr(self.cfg, 'gp_lambda', 10.0)  # Default lambda=10\n",
            "        gradient_penalty = self.compute_wgan_gradient_penalty(real_data, fake_data)\n",
            "        \n",
            "        # Total critic loss\n",
            "        total_critic_loss = critic_loss + gp_lambda * gradient_penalty\n",
            "        \n",
            "        # \"Accuracy\" metrics (for compatibility with logging)\n",
            "        # In WGAN, we use the sign of the output as a proxy\n",
            "        disc_acc_real = (d_real > 0).float().mean().item()\n",
            "        disc_acc_fake = (d_fake < 0).float().mean().item()\n",
            "        \n",
            "        # Backward pass\n",
            "        self.generator.train()\n",
            "        self.discriminator_opt.zero_grad()\n",
            "        self.accelerator.backward(total_critic_loss * self.cfg.loss_coefficient_disc)\n",
            "        self.accelerator.clip_grad_norm_(\n",
            "            self.discriminator.parameters(),\n",
            "            self.cfg.max_grad_norm\n",
            "        )\n",
            "        self.discriminator_opt.step()\n",
            "        self.discriminator_scheduler.step()\n",
            "        \n",
            "        return gradient_penalty.detach(), critic_loss.detach(), disc_acc_real, disc_acc_fake\n",
            "    \n",
            "    def _step_generator(self, real_data: torch.Tensor, fake_data: torch.Tensor) -> tuple[torch.Tensor, float]:\n",
            "        \"\"\"WGAN generator update step.\n",
            "        \n",
            "        Generator loss = -E[D(fake)]\n",
            "        Generator wants critic to think fake samples are real (high scores).\n",
            "        \"\"\"\n",
            "        # Get critic score for fake samples\n",
            "        d_fake = self.discriminator(fake_data)\n",
            "        \n",
            "        # Generator wants to maximize D(fake), so minimize -D(fake)\n",
            "        gen_loss = -d_fake.mean()\n",
            "        \n",
            "        # \"Accuracy\" metric (proxy)\n",
            "        gen_acc = (d_fake > 0).float().mean().item()\n",
            "        \n",
            "        return gen_loss, gen_acc\n",
            "'''\n",
            "\n",
            "# Check if WGAN is already added\n",
            "if 'class WassersteinGAN' not in gan_code:\n",
            "    # Add WGAN-GP implementation\n",
            "    with open(gan_path, 'a') as f:\n",
            "        f.write(wgan_code)\n",
            "    print(\"Added WassersteinGAN class to utils/gan.py\")\n",
            "else:\n",
            "    print(\"WassersteinGAN already exists in utils/gan.py\")\n",
            "\n",
            "# Now update train.py to support the wgan style\n",
            "train_path = '/content/vec2vec/train.py'\n",
            "with open(train_path, 'r') as f:\n",
            "    train_code = f.read()\n",
            "\n",
            "# Check if we need to add WGAN import and handling\n",
            "if 'WassersteinGAN' not in train_code:\n",
            "    # Update the imports\n",
            "    old_import = 'from utils.gan import LeastSquaresGAN, RelativisticGAN, VanillaGAN'\n",
            "    new_import = 'from utils.gan import LeastSquaresGAN, RelativisticGAN, VanillaGAN, WassersteinGAN'\n",
            "    train_code = train_code.replace(old_import, new_import)\n",
            "    \n",
            "    # Update the GAN style selection\n",
            "    old_selection = '''    if cfg.gan_style == \"vanilla\":\n",
            "        gan_cls = VanillaGAN\n",
            "    elif cfg.gan_style == \"least_squares\":\n",
            "        gan_cls = LeastSquaresGAN\n",
            "    elif cfg.gan_style == \"relativistic\":\n",
            "        gan_cls = RelativisticGAN\n",
            "    else:\n",
            "        raise ValueError(f\"Unknown GAN style: {cfg.gan_style}\")'''\n",
            "    \n",
            "    new_selection = '''    if cfg.gan_style == \"vanilla\":\n",
            "        gan_cls = VanillaGAN\n",
            "    elif cfg.gan_style == \"least_squares\":\n",
            "        gan_cls = LeastSquaresGAN\n",
            "    elif cfg.gan_style == \"relativistic\":\n",
            "        gan_cls = RelativisticGAN\n",
            "    elif cfg.gan_style == \"wgan\":\n",
            "        gan_cls = WassersteinGAN\n",
            "    else:\n",
            "        raise ValueError(f\"Unknown GAN style: {cfg.gan_style}\")'''\n",
            "    \n",
            "    train_code = train_code.replace(old_selection, new_selection)\n",
            "    \n",
            "    with open(train_path, 'w') as f:\n",
            "        f.write(train_code)\n",
            "    print(\"Updated train.py to support gan_style='wgan'\")\n",
            "else:\n",
            "    print(\"train.py already supports WassersteinGAN\")\n",
            "\n",
            "print(\"\\nCodebase modifications complete!\")"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 9: Markdown - Training configuration explanation
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Training Configuration\n",
            "\n",
            "We'll use the `unsupervised.toml` config as the base and run two experiments:\n",
            "\n",
            "### Run 1: Baseline GAN (Least Squares)\n",
            "- `gan_style = \"least_squares\"` (original vec2vec)\n",
            "- Standard discriminator with MSE-based loss\n",
            "\n",
            "### Run 2: WGAN-GP\n",
            "- `gan_style = \"wgan\"`\n",
            "- Wasserstein distance with gradient penalty\n",
            "- `gp_lambda = 10` (gradient penalty coefficient)\n",
            "\n",
            "### Shared Hyperparameters for Colab:\n",
            "- **num_points**: 25,000 (samples per encoder, 50k total)\n",
            "- **epochs**: 8 (reduced for faster iteration)\n",
            "- **batch_size**: 128 (reduced for GPU memory)\n",
            "- **learning_rate**: 2e-5 (default)\n",
            "- **mixed_precision**: fp16 (for memory efficiency)\n",
            "\n",
            "All other components (architecture, reconstruction loss, VSP loss, etc.) remain identical."
        ]
    })

    # Cell 10: Code - Show/define configs for runs
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Display and define configurations for both runs\n",
            "import toml\n",
            "import os\n",
            "\n",
            "# Load base config\n",
            "config_path = '/content/vec2vec/configs/unsupervised.toml'\n",
            "with open(config_path, 'r') as f:\n",
            "    config = toml.load(f)\n",
            "\n",
            "print(\"=\" * 70)\n",
            "print(\"BASE CONFIGURATION (unsupervised.toml)\")\n",
            "print(\"=\" * 70)\n",
            "\n",
            "# Key settings\n",
            "print(f\"\\n[General]\")\n",
            "print(f\"  Dataset: {config['general']['dataset']}\")\n",
            "print(f\"  Unsup Model: {config['general']['unsup_emb']}\")\n",
            "print(f\"  Sup Model: {config['general']['sup_emb']}\")\n",
            "\n",
            "print(f\"\\n[Translator]\")\n",
            "print(f\"  Style: {config['translator']['style']}\")\n",
            "print(f\"  Adapter Dim: {config['translator']['d_adapter']}\")\n",
            "\n",
            "print(f\"\\n[Discriminator]\")\n",
            "print(f\"  GAN Style: {config['discriminator']['gan_style']}\")\n",
            "print(f\"  Depth: {config['discriminator']['disc_depth']}\")\n",
            "\n",
            "# Define shared Colab settings\n",
            "SHARED_CONFIG = {\n",
            "    'num_points': 25000,       # 25k per encoder (50k total)\n",
            "    'epochs': 8,               # Reduced for Colab\n",
            "    'bs': 128,                 # Reduced batch size for T4\n",
            "    'val_size': 4096,          # Validation set size\n",
            "    'use_wandb': False,        # Disable W&B for simplicity\n",
            "    'min_epochs': 4,           # Lower minimum epochs\n",
            "    'patience': 3,             # Earlier stopping\n",
            "}\n",
            "\n",
            "print(\"\\n\" + \"=\" * 70)\n",
            "print(\"SHARED COLAB SETTINGS\")\n",
            "print(\"=\" * 70)\n",
            "for k, v in SHARED_CONFIG.items():\n",
            "    print(f\"  --{k} {v}\")\n",
            "\n",
            "print(\"\\n\" + \"=\" * 70)\n",
            "print(\"EXPERIMENT CONFIGURATIONS\")\n",
            "print(\"=\" * 70)\n",
            "\n",
            "print(\"\\n[Run 1: Baseline GAN (Least Squares)]\")\n",
            "print(\"  --gan_style least_squares\")\n",
            "print(\"  Output: outputs/vec2vec_colab_50k_gan/\")\n",
            "\n",
            "print(\"\\n[Run 2: WGAN-GP]\")\n",
            "print(\"  --gan_style wgan\")\n",
            "print(\"  --gp_lambda 10\")\n",
            "print(\"  Output: outputs/vec2vec_colab_50k_wgan/\")\n",
            "\n",
            "print(\"\\n\" + \"=\" * 70)"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 11: Code - Launch training for baseline GAN
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Run 1: Train with baseline GAN (Least Squares)\n",
            "import os\n",
            "os.chdir('/content/vec2vec')\n",
            "\n",
            "# Create output directory\n",
            "OUTPUT_DIR_GAN = '/content/vec2vec/outputs/vec2vec_colab_50k_gan'\n",
            "os.makedirs(OUTPUT_DIR_GAN, exist_ok=True)\n",
            "\n",
            "# Build training command\n",
            "CMD_GAN = f\"\"\"\n",
            "python train.py unsupervised \\\\\n",
            "    --num_points 25000 \\\\\n",
            "    --epochs 8 \\\\\n",
            "    --bs 128 \\\\\n",
            "    --val_size 4096 \\\\\n",
            "    --use_wandb false \\\\\n",
            "    --min_epochs 4 \\\\\n",
            "    --patience 3 \\\\\n",
            "    --gan_style least_squares \\\\\n",
            "    --save_dir '{OUTPUT_DIR_GAN}/{{}}' \\\\\n",
            "    --force_wandb_name true \\\\\n",
            "    --wandb_name gan_baseline\n",
            "\"\"\"\n",
            "\n",
            "print(\"=\" * 70)\n",
            "print(\"RUN 1: BASELINE GAN (Least Squares)\")\n",
            "print(\"=\" * 70)\n",
            "print(\"\\nTraining command:\")\n",
            "print(CMD_GAN)\n",
            "print(\"\\nStarting training... (this may take 30-60 minutes on a T4)\")\n",
            "print(\"=\" * 70 + \"\\n\")\n",
            "\n",
            "# Run training\n",
            "!{CMD_GAN}"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 12: Code - Launch training for WGAN
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Run 2: Train with WGAN-GP\n",
            "import os\n",
            "os.chdir('/content/vec2vec')\n",
            "\n",
            "# Create output directory\n",
            "OUTPUT_DIR_WGAN = '/content/vec2vec/outputs/vec2vec_colab_50k_wgan'\n",
            "os.makedirs(OUTPUT_DIR_WGAN, exist_ok=True)\n",
            "\n",
            "# Build training command\n",
            "CMD_WGAN = f\"\"\"\n",
            "python train.py unsupervised \\\\\n",
            "    --num_points 25000 \\\\\n",
            "    --epochs 8 \\\\\n",
            "    --bs 128 \\\\\n",
            "    --val_size 4096 \\\\\n",
            "    --use_wandb false \\\\\n",
            "    --min_epochs 4 \\\\\n",
            "    --patience 3 \\\\\n",
            "    --gan_style wgan \\\\\n",
            "    --gp_lambda 10 \\\\\n",
            "    --save_dir '{OUTPUT_DIR_WGAN}/{{}}' \\\\\n",
            "    --force_wandb_name true \\\\\n",
            "    --wandb_name wgan_gp\n",
            "\"\"\"\n",
            "\n",
            "print(\"=\" * 70)\n",
            "print(\"RUN 2: WGAN-GP\")\n",
            "print(\"=\" * 70)\n",
            "print(\"\\nTraining command:\")\n",
            "print(CMD_WGAN)\n",
            "print(\"\\nStarting training... (this may take 30-60 minutes on a T4)\")\n",
            "print(\"=\" * 70 + \"\\n\")\n",
            "\n",
            "# Run training\n",
            "!{CMD_WGAN}"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 13: Markdown - Evaluation explanation
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Evaluation\n",
            "\n",
            "We evaluate both trained translators on the same held-out test data using the following metrics:\n",
            "\n",
            "1. **Mean Cosine Similarity**: Average cosine similarity between translated embeddings and true target embeddings (higher is better)\n",
            "2. **Top-1 Accuracy**: Percentage of samples where the translated embedding's nearest neighbor is the correct target (higher is better)\n",
            "3. **Mean Rank**: Average rank of the correct target among all candidates (lower is better)\n",
            "4. **VSP (Vector Space Preservation)**: How well pairwise similarities are preserved after translation (lower is better)\n",
            "\n",
            "The comparison will show whether WGAN-GP improves translation quality over the standard least-squares GAN."
        ]
    })

    # Cell 14: Code - Evaluation for both modes
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Evaluate both models and compare results\n",
            "import os\n",
            "import sys\n",
            "import glob\n",
            "import json\n",
            "import torch\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from types import SimpleNamespace\n",
            "\n",
            "sys.path.insert(0, '/content/vec2vec')\n",
            "os.chdir('/content/vec2vec')\n",
            "\n",
            "import toml\n",
            "import accelerate\n",
            "from torch.utils.data import DataLoader\n",
            "\n",
            "from utils.collate import MultiencoderTokenizedDataset, TokenizedCollator\n",
            "from utils.eval_utils import eval_loop_, create_heatmap\n",
            "from utils.model_utils import get_sentence_embedding_dimension, load_encoder\n",
            "from utils.utils import load_n_translator, get_num_proc\n",
            "from utils.streaming_utils import load_streaming_embeddings, process_batch\n",
            "\n",
            "def find_checkpoint(base_dir):\n",
            "    \"\"\"Find the model checkpoint directory.\"\"\"\n",
            "    checkpoint_dirs = glob.glob(f\"{base_dir}/**/model.pt\", recursive=True)\n",
            "    if checkpoint_dirs:\n",
            "        return os.path.dirname(sorted(checkpoint_dirs, key=os.path.getmtime)[-1])\n",
            "    return None\n",
            "\n",
            "def evaluate_model(checkpoint_path, test_size=4096):\n",
            "    \"\"\"Evaluate a trained model and return metrics.\"\"\"\n",
            "    if not checkpoint_path:\n",
            "        return None\n",
            "    \n",
            "    # Load config\n",
            "    config_file = os.path.join(checkpoint_path, 'config.toml')\n",
            "    if not os.path.exists(config_file):\n",
            "        print(f\"Config not found: {config_file}\")\n",
            "        return None\n",
            "    \n",
            "    cfg = SimpleNamespace(**toml.load(config_file))\n",
            "    \n",
            "    # Setup accelerator\n",
            "    accelerator = accelerate.Accelerator(\n",
            "        mixed_precision=cfg.mixed_precision if hasattr(cfg, 'mixed_precision') else None\n",
            "    )\n",
            "    accelerator.dataloader_config.dispatch_batches = False\n",
            "    \n",
            "    # Load encoders\n",
            "    sup_encs = {cfg.sup_emb: load_encoder(cfg.sup_emb, mixed_precision=getattr(cfg, 'mixed_precision', None))}\n",
            "    unsup_enc = {cfg.unsup_emb: load_encoder(cfg.unsup_emb, mixed_precision=getattr(cfg, 'mixed_precision', None))}\n",
            "    \n",
            "    # Load translator\n",
            "    encoder_dims = {cfg.sup_emb: get_sentence_embedding_dimension(sup_encs[cfg.sup_emb])}\n",
            "    translator = load_n_translator(cfg, encoder_dims)\n",
            "    unsup_dim = {cfg.unsup_emb: get_sentence_embedding_dimension(unsup_enc[cfg.unsup_emb])}\n",
            "    translator.add_encoders(unsup_dim, overwrite_embs=[cfg.unsup_emb])\n",
            "    \n",
            "    # Load weights\n",
            "    translator.load_state_dict(torch.load(os.path.join(checkpoint_path, 'model.pt'), map_location='cpu'), strict=False)\n",
            "    translator = accelerator.prepare(translator)\n",
            "    translator.eval()\n",
            "    \n",
            "    # Load test data\n",
            "    dset = load_streaming_embeddings(cfg.dataset)\n",
            "    dset_dict = dset.train_test_split(test_size=test_size, seed=cfg.val_dataset_seed)\n",
            "    testset = dset_dict[\"test\"]\n",
            "    \n",
            "    num_workers = min(get_num_proc(), 4)\n",
            "    evalset = MultiencoderTokenizedDataset(\n",
            "        dataset=testset,\n",
            "        encoders={**unsup_enc, **sup_encs},\n",
            "        n_embs_per_batch=2,\n",
            "        batch_size=cfg.val_bs if hasattr(cfg, 'val_bs') else 256,\n",
            "        max_length=cfg.max_seq_length,\n",
            "        seed=cfg.sampling_seed,\n",
            "    )\n",
            "    evalloader = DataLoader(\n",
            "        evalset,\n",
            "        batch_size=cfg.val_bs if hasattr(cfg, 'val_bs') else 256,\n",
            "        num_workers=num_workers,\n",
            "        shuffle=False,\n",
            "        pin_memory=True,\n",
            "        collate_fn=TokenizedCollator(),\n",
            "        drop_last=True,\n",
            "    )\n",
            "    evalloader = accelerator.prepare(evalloader)\n",
            "    \n",
            "    # Run evaluation\n",
            "    with torch.no_grad():\n",
            "        recons, trans, heatmap_dict, _, _, _ = eval_loop_(\n",
            "            cfg, translator, {**sup_encs, **unsup_enc}, evalloader, device=accelerator.device\n",
            "        )\n",
            "    \n",
            "    # Extract metrics\n",
            "    metrics = {\n",
            "        'gan_style': cfg.gan_style,\n",
            "        'train_size': cfg.num_points,\n",
            "        'test_size': test_size,\n",
            "    }\n",
            "    \n",
            "    # Get translation metrics (unsup -> sup)\n",
            "    trans_key = f\"{cfg.unsup_emb}_{cfg.sup_emb}\"\n",
            "    if cfg.sup_emb in trans and cfg.unsup_emb in trans[cfg.sup_emb]:\n",
            "        t = trans[cfg.sup_emb][cfg.unsup_emb]\n",
            "        metrics['cosine'] = t.get('cos', 0)\n",
            "        metrics['vsp'] = t.get('vsp', 0)\n",
            "    \n",
            "    # Get top-1 and rank from heatmap_dict\n",
            "    for k, v in heatmap_dict.items():\n",
            "        if f\"{cfg.unsup_emb}_{cfg.sup_emb}_top_1_acc\" in k:\n",
            "            metrics['top1'] = v\n",
            "        elif f\"{cfg.unsup_emb}_{cfg.sup_emb}_rank\" in k and 'var' not in k:\n",
            "            metrics['rank'] = v\n",
            "    \n",
            "    return metrics\n",
            "\n",
            "# Find checkpoints\n",
            "gan_checkpoint = find_checkpoint('/content/vec2vec/outputs/vec2vec_colab_50k_gan')\n",
            "wgan_checkpoint = find_checkpoint('/content/vec2vec/outputs/vec2vec_colab_50k_wgan')\n",
            "\n",
            "print(\"=\" * 70)\n",
            "print(\"EVALUATING TRAINED MODELS\")\n",
            "print(\"=\" * 70)\n",
            "\n",
            "results = []\n",
            "\n",
            "# Evaluate GAN baseline\n",
            "if gan_checkpoint:\n",
            "    print(f\"\\nEvaluating GAN baseline: {gan_checkpoint}\")\n",
            "    gan_metrics = evaluate_model(gan_checkpoint)\n",
            "    if gan_metrics:\n",
            "        gan_metrics['adv_type'] = 'GAN (LS)'\n",
            "        results.append(gan_metrics)\n",
            "        print(f\"  Done!\")\n",
            "else:\n",
            "    print(\"\\nGAN baseline checkpoint not found\")\n",
            "\n",
            "# Evaluate WGAN\n",
            "if wgan_checkpoint:\n",
            "    print(f\"\\nEvaluating WGAN-GP: {wgan_checkpoint}\")\n",
            "    wgan_metrics = evaluate_model(wgan_checkpoint)\n",
            "    if wgan_metrics:\n",
            "        wgan_metrics['adv_type'] = 'WGAN-GP'\n",
            "        results.append(wgan_metrics)\n",
            "        print(f\"  Done!\")\n",
            "else:\n",
            "    print(\"\\nWGAN-GP checkpoint not found\")\n",
            "\n",
            "print(\"\\n\" + \"=\" * 70)"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 15: Code - Create comparison table
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Create comparison table\n",
            "import pandas as pd\n",
            "\n",
            "if results:\n",
            "    # Create DataFrame\n",
            "    df = pd.DataFrame(results)\n",
            "    \n",
            "    # Reorder columns\n",
            "    cols = ['adv_type', 'train_size', 'test_size', 'cosine', 'top1', 'rank', 'vsp']\n",
            "    df = df[[c for c in cols if c in df.columns]]\n",
            "    \n",
            "    # Rename for display\n",
            "    df.columns = ['Adv Type', 'Train Size', 'Test Size', 'Cosine', 'Top-1', 'Rank', 'VSP']\n",
            "    \n",
            "    print(\"\\n\" + \"=\" * 70)\n",
            "    print(\"COMPARISON RESULTS\")\n",
            "    print(\"=\" * 70)\n",
            "    print(\"\\nModel pair: stella -> gte (unsupervised -> supervised)\")\n",
            "    print(\"Dataset: Natural Questions (NQ)\")\n",
            "    print(\"\\n\")\n",
            "    \n",
            "    # Format the table\n",
            "    pd.set_option('display.float_format', lambda x: '%.4f' % x if abs(x) < 100 else '%.1f' % x)\n",
            "    print(df.to_string(index=False))\n",
            "    \n",
            "    print(\"\\n\")\n",
            "    print(\"Metrics explanation:\")\n",
            "    print(\"  Cosine: Mean cosine similarity (higher is better)\")\n",
            "    print(\"  Top-1:  Nearest neighbor accuracy (higher is better)\")\n",
            "    print(\"  Rank:   Mean rank of correct target (lower is better)\")\n",
            "    print(\"  VSP:    Vector space preservation error (lower is better)\")\n",
            "    print(\"\\n\" + \"=\" * 70)\n",
            "    \n",
            "    # Save results\n",
            "    results_path = '/content/vec2vec/outputs/comparison_results.csv'\n",
            "    df.to_csv(results_path, index=False)\n",
            "    print(f\"\\nResults saved to: {results_path}\")\n",
            "else:\n",
            "    print(\"\\nNo results to compare. Please run both training cells first.\")"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 16: Code - Plots
    cells.append({
        "cell_type": "code",
        "metadata": {},
        "source": [
            "# Create comparison plots\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "if results and len(results) >= 2:\n",
            "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
            "    \n",
            "    # Extract data\n",
            "    labels = [r['adv_type'] for r in results]\n",
            "    colors = ['#2ecc71', '#3498db']  # Green for GAN, Blue for WGAN\n",
            "    \n",
            "    # Plot 1: Cosine Similarity\n",
            "    if 'cosine' in results[0]:\n",
            "        values = [r.get('cosine', 0) for r in results]\n",
            "        bars = axes[0].bar(labels, values, color=colors)\n",
            "        axes[0].set_ylabel('Cosine Similarity')\n",
            "        axes[0].set_title('Cosine Similarity (higher is better)')\n",
            "        axes[0].set_ylim([min(values) * 0.95, max(values) * 1.02])\n",
            "        for bar, val in zip(bars, values):\n",
            "            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
            "                        f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
            "    \n",
            "    # Plot 2: Top-1 Accuracy\n",
            "    if 'top1' in results[0]:\n",
            "        values = [r.get('top1', 0) for r in results]\n",
            "        bars = axes[1].bar(labels, values, color=colors)\n",
            "        axes[1].set_ylabel('Top-1 Accuracy')\n",
            "        axes[1].set_title('Top-1 Accuracy (higher is better)')\n",
            "        axes[1].set_ylim([0, max(values) * 1.2])\n",
            "        for bar, val in zip(bars, values):\n",
            "            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
            "                        f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
            "    \n",
            "    # Plot 3: Mean Rank\n",
            "    if 'rank' in results[0]:\n",
            "        values = [r.get('rank', 0) for r in results]\n",
            "        bars = axes[2].bar(labels, values, color=colors)\n",
            "        axes[2].set_ylabel('Mean Rank')\n",
            "        axes[2].set_title('Mean Rank (lower is better)')\n",
            "        axes[2].set_ylim([0, max(values) * 1.3])\n",
            "        for bar, val in zip(bars, values):\n",
            "            axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
            "                        f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
            "    \n",
            "    # Plot 4: VSP Error\n",
            "    if 'vsp' in results[0]:\n",
            "        values = [r.get('vsp', 0) for r in results]\n",
            "        bars = axes[3].bar(labels, values, color=colors)\n",
            "        axes[3].set_ylabel('VSP Error')\n",
            "        axes[3].set_title('VSP Error (lower is better)')\n",
            "        axes[3].set_ylim([0, max(values) * 1.3])\n",
            "        for bar, val in zip(bars, values):\n",
            "            axes[3].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
            "                        f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
            "    \n",
            "    plt.tight_layout()\n",
            "    plt.savefig('/content/vec2vec/outputs/comparison_plots.png', dpi=150, bbox_inches='tight')\n",
            "    plt.show()\n",
            "    \n",
            "    print(\"\\nPlots saved to: /content/vec2vec/outputs/comparison_plots.png\")\n",
            "else:\n",
            "    print(\"Need results from both runs to create comparison plots.\")"
        ],
        "execution_count": None,
        "outputs": []
    })

    # Cell 17: Markdown - Summary & discussion
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## Summary & Discussion\n",
            "\n",
            "### What This Notebook Reproduces\n",
            "\n",
            "This notebook trains and compares two vec2vec translators:\n",
            "\n",
            "1. **Baseline (GAN)**: Original vec2vec with Least Squares GAN loss\n",
            "2. **WGAN-GP**: Vec2vec with Wasserstein GAN + Gradient Penalty\n",
            "\n",
            "Both use:\n",
            "- **50k training examples** (25k per encoder)\n",
            "- **Stella -> GTE** model pair (unsupervised -> supervised)\n",
            "- **ResNet MLP architecture** with adapters\n",
            "- Identical reconstruction, VSP, and cross-chain losses\n",
            "\n",
            "Only the adversarial component differs:\n",
            "- GAN: `L_D = 0.5 * (D(real)^2 + (D(fake)-1)^2)`, `L_G = 0.5 * D(fake)^2`\n",
            "- WGAN: `L_C = E[D(fake)] - E[D(real)] + 10 * GP`, `L_G = -E[D(fake)]`\n",
            "\n",
            "---\n",
            "\n",
            "### Interpreting Results\n",
            "\n",
            "Key questions to consider:\n",
            "\n",
            "1. **Did WGAN-GP improve cosine similarity?**\n",
            "   - Higher cosine = better alignment with target embeddings\n",
            "\n",
            "2. **Did Top-1 accuracy change?**\n",
            "   - Measures exact retrieval performance\n",
            "\n",
            "3. **Did mean rank improve?**\n",
            "   - Lower rank = better overall ranking quality\n",
            "\n",
            "4. **Was training more stable?**\n",
            "   - WGAN-GP typically provides smoother gradients\n",
            "\n",
            "---\n",
            "\n",
            "### Tuning WGAN-GP\n",
            "\n",
            "If WGAN-GP underperforms, try:\n",
            "\n",
            "```python\n",
            "# Adjust gradient penalty coefficient\n",
            "--gp_lambda 1     # Less regularization\n",
            "--gp_lambda 100   # More regularization\n",
            "\n",
            "# Adjust critic learning rate\n",
            "--disc_lr 5e-5    # Higher for WGAN\n",
            "\n",
            "# Multiple critic steps per generator step (not implemented here)\n",
            "# Would require modifying training loop\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Scaling Up\n",
            "\n",
            "To increase training scale:\n",
            "\n",
            "```python\n",
            "# More training data\n",
            "--num_points 100000\n",
            "\n",
            "# More epochs\n",
            "--epochs 30\n",
            "\n",
            "# Larger batch size (if GPU memory allows)\n",
            "--bs 256\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Changing Configurations\n",
            "\n",
            "**Different model pairs:**\n",
            "```python\n",
            "--unsup_emb gte --sup_emb gtr    # GTE to GTR\n",
            "--unsup_emb e5 --sup_emb gte     # E5 to GTE\n",
            "```\n",
            "\n",
            "**Different datasets:**\n",
            "```python\n",
            "--dataset fineweb\n",
            "--dataset msmarco-corpus\n",
            "```\n",
            "\n",
            "**Toggle adversarial modes:**\n",
            "```python\n",
            "--gan_style vanilla         # Standard BCE GAN\n",
            "--gan_style least_squares   # LSGAN (default)\n",
            "--gan_style relativistic    # Relativistic GAN\n",
            "--gan_style wgan            # WGAN-GP\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### Citation\n",
            "\n",
            "```bibtex\n",
            "@misc{jha2025harnessinguniversalgeometryembeddings,\n",
            "      title={Harnessing the Universal Geometry of Embeddings}, \n",
            "      author={Rishi Jha and Collin Zhang and Vitaly Shmatikov and John X. Morris},\n",
            "      year={2025},\n",
            "      eprint={2505.12540},\n",
            "      archivePrefix={arXiv},\n",
            "      primaryClass={cs.LG},\n",
            "      url={https://arxiv.org/abs/2505.12540}, \n",
            "}\n",
            "```"
        ]
    })

    # Build notebook structure
    notebook = {
        "nbformat": 4,
        "nbformat_minor": 4,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.10.0"
            },
            "accelerator": "GPU",
            "colab": {
                "provenance": [],
                "gpuType": "T4"
            }
        },
        "cells": cells
    }

    # Write notebook
    output_path = '/home/user/vec2vec/vec2vec_colab_50k_wgan.ipynb'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=2, ensure_ascii=False)

    print(f"Notebook created: {output_path}")
    return output_path

if __name__ == "__main__":
    create_notebook()
