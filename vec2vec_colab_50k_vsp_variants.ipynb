{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0da3cd",
   "metadata": {},
   "source": [
    "# vec2vec: Colab Reproduction with VSP Variants (~50k Examples)\n",
    "\n",
    "This notebook reproduces the core vec2vec model from the paper [\"Harnessing the Universal Geometry of Embeddings\"](https://arxiv.org/abs/2505.12540) with modified VSP (Vector Space Preservation) variants.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "We train vec2vec on **stella → gte** embedding translation using the **NQ dataset** with ~50,000 training examples, comparing three VSP loss variants:\n",
    "\n",
    "1. **Original VSP** - Preserves pairwise dot-product similarities (as in the paper)\n",
    "2. **Conformal VSP** - Preserves angles (cosine similarities) between vectors\n",
    "3. **Topological VSP (kNN)** - Preserves k-nearest neighbor structure\n",
    "\n",
    "## Goal\n",
    "\n",
    "Explore whether the \"universal geometry\" of embeddings is:\n",
    "- **Metric** (dot-product preservation - original)\n",
    "- **Conformal** (angle preservation)\n",
    "- **Topological** (neighborhood preservation)\n",
    "\n",
    "## Metrics\n",
    "\n",
    "For each variant, we evaluate:\n",
    "- **Cosine Similarity** - Alignment between translated and ground-truth embeddings\n",
    "- **Top-1 Accuracy** - Nearest neighbor retrieval accuracy\n",
    "- **Mean Rank** - Average rank of correct target in similarity-ranked list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fbca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and system info\n",
    "!nvidia-smi\n",
    "\n",
    "import sys\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not yet installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9496c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the vec2vec repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/danielyxu/vec2vec.git\"\n",
    "REPO_DIR = \"/content/vec2vec\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "!git pull origin main\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabd8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets accelerate wandb safetensors toml sentence-transformers\n",
    "!pip install -q pandas matplotlib seaborn tqdm\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"Datasets: {datasets.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29a48b",
   "metadata": {},
   "source": [
    "## Data & Embedding Preparation\n",
    "\n",
    "The vec2vec framework uses **streaming embeddings** from pre-encoded text datasets. For this notebook:\n",
    "\n",
    "- **Dataset**: Natural Questions (NQ) - a question-answering dataset\n",
    "- **Source Embedding**: `stella` (infgrad/stella-base-en-v2) - 768 dimensions\n",
    "- **Target Embedding**: `gte` (thenlper/gte-base) - 768 dimensions\n",
    "\n",
    "We'll use:\n",
    "- **~50,000 training examples** (subset of full dataset)\n",
    "- **4,096 validation examples**\n",
    "- **8,192 test examples** for final evaluation\n",
    "\n",
    "The data loading happens on-the-fly during training using HuggingFace datasets streaming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration - no pre-download needed, data streams during training\n",
    "# The vec2vec repo handles data loading via HuggingFace datasets\n",
    "\n",
    "# Verify we can import the data utilities\n",
    "import sys\n",
    "sys.path.insert(0, '/content/vec2vec')\n",
    "\n",
    "from utils.streaming_utils import load_streaming_embeddings\n",
    "from utils.model_utils import load_encoder, get_sentence_embedding_dimension\n",
    "\n",
    "# Test data loading\n",
    "print(\"Testing data loading...\")\n",
    "try:\n",
    "    dset = load_streaming_embeddings(\"nq\")\n",
    "    print(f\"✓ NQ dataset loaded successfully\")\n",
    "    print(f\"  Dataset features: {list(dset.features.keys())[:5]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Data loading error: {e}\")\n",
    "    print(\"  Will attempt during training...\")\n",
    "\n",
    "# Check embedding model dimensions\n",
    "print(f\"\\nEmbedding dimensions:\")\n",
    "print(f\"  stella: {get_sentence_embedding_dimension('stella')}\")\n",
    "print(f\"  gte: {get_sentence_embedding_dimension('gte')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d5364",
   "metadata": {},
   "source": [
    "## VSP Loss Implementation Overview\n",
    "\n",
    "### Original VSP (from paper)\n",
    "Located in `utils/train_utils.py`, the original VSP preserves pairwise similarities:\n",
    "\n",
    "```python\n",
    "# Normalize embeddings\n",
    "B = target / target.norm(dim=1, keepdim=True)  # Target space\n",
    "A = translated / translated.norm(dim=1, keepdim=True)  # Translated\n",
    "\n",
    "# Compute similarity matrices\n",
    "S_target = B @ B.T\n",
    "S_translated = A @ A.T\n",
    "S_cross = A @ B.T\n",
    "\n",
    "# VSP loss = MAE between similarity matrices\n",
    "vsp_loss = |S_target - S_translated|.mean() + |S_target - S_cross|.mean()\n",
    "```\n",
    "\n",
    "### Our Variants\n",
    "\n",
    "1. **Conformal VSP**: Preserves angles (cosine similarities) more explicitly\n",
    "   - Normalizes similarity matrices before comparison\n",
    "   - Focuses on angular relationships\n",
    "\n",
    "2. **Topological VSP (kNN)**: Preserves neighborhood structure\n",
    "   - For each point, computes k-nearest neighbors in both spaces\n",
    "   - Penalizes disagreement in neighbor sets (Jaccard similarity)\n",
    "\n",
    "3. **Topological VSP (Soft)**: Soft neighborhood preservation\n",
    "   - Converts distances to probability distributions\n",
    "   - Minimizes KL divergence between neighbor distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eea021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extended VSP loss functions with multiple variants\n",
    "\n",
    "vsp_variants_code = \"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def vsp_loss_original(ins, translations, logger=None) -> torch.Tensor:\n",
    "    # Original VSP loss from paper - preserves pairwise similarities.\n",
    "    loss = None\n",
    "    EPS = 1e-10\n",
    "    count = 0\n",
    "\n",
    "    for out_name in ins.keys():\n",
    "        for in_name in translations[out_name].keys():\n",
    "            B = ins[out_name].detach()\n",
    "            B = B / (B.norm(dim=1, keepdim=True) + EPS)\n",
    "            in_sims = B @ B.T\n",
    "\n",
    "            A = translations[out_name][in_name]\n",
    "            A = A / (A.norm(dim=1, keepdim=True) + EPS)\n",
    "            out_sims = A @ A.T\n",
    "            out_sims_reflected = A @ B.T\n",
    "\n",
    "            vsp_loss = (in_sims - out_sims).abs().mean()\n",
    "            vsp_loss_reflected = (in_sims - out_sims_reflected).abs().mean()\n",
    "\n",
    "            if loss is None:\n",
    "                loss = vsp_loss + vsp_loss_reflected\n",
    "            else:\n",
    "                loss += vsp_loss + vsp_loss_reflected\n",
    "            count += 1\n",
    "\n",
    "    return loss / count if count > 0 else torch.tensor(0.0)\n",
    "\n",
    "\n",
    "def vsp_loss_conformal(ins, translations, logger=None) -> torch.Tensor:\n",
    "    # Conformal VSP - preserves angles (cosine similarities) explicitly.\n",
    "    loss = None\n",
    "    EPS = 1e-10\n",
    "    count = 0\n",
    "\n",
    "    for out_name in ins.keys():\n",
    "        for in_name in translations[out_name].keys():\n",
    "            B = ins[out_name].detach()\n",
    "            B = B / (B.norm(dim=1, keepdim=True) + EPS)\n",
    "\n",
    "            A = translations[out_name][in_name]\n",
    "            A = A / (A.norm(dim=1, keepdim=True) + EPS)\n",
    "\n",
    "            cos_sim_target = B @ B.T\n",
    "            cos_sim_trans = A @ A.T\n",
    "\n",
    "            cos_sim_target_norm = cos_sim_target / (cos_sim_target.abs().max() + EPS)\n",
    "            cos_sim_trans_norm = cos_sim_trans / (cos_sim_trans.abs().max() + EPS)\n",
    "\n",
    "            conformal_loss = F.mse_loss(cos_sim_trans_norm, cos_sim_target_norm)\n",
    "\n",
    "            cos_sim_cross = A @ B.T\n",
    "            cos_sim_cross_norm = cos_sim_cross / (cos_sim_cross.abs().max() + EPS)\n",
    "            cross_loss = F.mse_loss(cos_sim_cross_norm, cos_sim_target_norm)\n",
    "\n",
    "            if loss is None:\n",
    "                loss = conformal_loss + cross_loss\n",
    "            else:\n",
    "                loss += conformal_loss + cross_loss\n",
    "            count += 1\n",
    "\n",
    "    return loss / count if count > 0 else torch.tensor(0.0)\n",
    "\n",
    "\n",
    "def vsp_loss_topo_knn(ins, translations, logger=None, k=5) -> torch.Tensor:\n",
    "    # Topological VSP (kNN) - preserves k-nearest neighbor structure.\n",
    "    loss = None\n",
    "    EPS = 1e-10\n",
    "    count = 0\n",
    "\n",
    "    for out_name in ins.keys():\n",
    "        for in_name in translations[out_name].keys():\n",
    "            B = ins[out_name].detach()\n",
    "            B = B / (B.norm(dim=1, keepdim=True) + EPS)\n",
    "\n",
    "            A = translations[out_name][in_name]\n",
    "            A = A / (A.norm(dim=1, keepdim=True) + EPS)\n",
    "\n",
    "            batch_size = B.shape[0]\n",
    "            k_actual = min(k, batch_size - 1)\n",
    "\n",
    "            sim_target = B @ B.T\n",
    "            sim_trans = A @ A.T\n",
    "\n",
    "            mask = torch.eye(batch_size, device=B.device).bool()\n",
    "            sim_target = sim_target.masked_fill(mask, -float('inf'))\n",
    "            sim_trans = sim_trans.masked_fill(mask, -float('inf'))\n",
    "\n",
    "            _, knn_target = sim_target.topk(k_actual, dim=1)\n",
    "            _, knn_trans = sim_trans.topk(k_actual, dim=1)\n",
    "\n",
    "            jaccard_sum = 0.0\n",
    "            for i in range(batch_size):\n",
    "                set_target = set(knn_target[i].tolist())\n",
    "                set_trans = set(knn_trans[i].tolist())\n",
    "                intersection = len(set_target & set_trans)\n",
    "                union = len(set_target | set_trans)\n",
    "                jaccard = intersection / union if union > 0 else 1.0\n",
    "                jaccard_sum += jaccard\n",
    "\n",
    "            topo_loss = 1.0 - (jaccard_sum / batch_size)\n",
    "\n",
    "            if loss is None:\n",
    "                loss = torch.tensor(topo_loss, device=A.device)\n",
    "            else:\n",
    "                loss = loss + topo_loss\n",
    "            count += 1\n",
    "\n",
    "    return loss / count if count > 0 else torch.tensor(0.0, device=next(iter(ins.values())).device)\n",
    "\n",
    "\n",
    "def vsp_loss_topo_soft(ins, translations, logger=None, temperature=0.1) -> torch.Tensor:\n",
    "    # Soft topological VSP - preserves neighborhood distributions via KL divergence.\n",
    "    loss = None\n",
    "    EPS = 1e-10\n",
    "    count = 0\n",
    "\n",
    "    for out_name in ins.keys():\n",
    "        for in_name in translations[out_name].keys():\n",
    "            B = ins[out_name].detach()\n",
    "            B = B / (B.norm(dim=1, keepdim=True) + EPS)\n",
    "\n",
    "            A = translations[out_name][in_name]\n",
    "            A = A / (A.norm(dim=1, keepdim=True) + EPS)\n",
    "\n",
    "            batch_size = B.shape[0]\n",
    "\n",
    "            sim_target = B @ B.T\n",
    "            sim_trans = A @ A.T\n",
    "\n",
    "            mask = torch.eye(batch_size, device=B.device).bool()\n",
    "            sim_target = sim_target.masked_fill(mask, -float('inf'))\n",
    "            sim_trans = sim_trans.masked_fill(mask, -float('inf'))\n",
    "\n",
    "            prob_target = F.softmax(sim_target / temperature, dim=1)\n",
    "            prob_trans = F.softmax(sim_trans / temperature, dim=1)\n",
    "\n",
    "            kl_loss = F.kl_div(prob_trans.log(), prob_target, reduction='batchmean')\n",
    "\n",
    "            if loss is None:\n",
    "                loss = kl_loss\n",
    "            else:\n",
    "                loss += kl_loss\n",
    "            count += 1\n",
    "\n",
    "    return loss / count if count > 0 else torch.tensor(0.0)\n",
    "\n",
    "\n",
    "VSP_LOSS_REGISTRY = {\n",
    "    'original': vsp_loss_original,\n",
    "    'conformal': vsp_loss_conformal,\n",
    "    'topo_knn': vsp_loss_topo_knn,\n",
    "    'topo_soft': vsp_loss_topo_soft,\n",
    "}\n",
    "\n",
    "def get_vsp_loss_fn(vsp_type='original'):\n",
    "    # Get VSP loss function by type.\n",
    "    if vsp_type not in VSP_LOSS_REGISTRY:\n",
    "        raise ValueError(f\"Unknown VSP type: {vsp_type}. Available: {list(VSP_LOSS_REGISTRY.keys())}\")\n",
    "    return VSP_LOSS_REGISTRY[vsp_type]\n",
    "\"\"\"\n",
    "\n",
    "# Save the VSP variants to a file\n",
    "with open('/content/vec2vec/utils/vsp_variants.py', 'w') as f:\n",
    "    f.write(vsp_variants_code)\n",
    "\n",
    "print(\"Created utils/vsp_variants.py with VSP loss variants:\")\n",
    "print(\"  - original: Standard dot-product preservation\")\n",
    "print(\"  - conformal: Angle/cosine similarity preservation\")\n",
    "print(\"  - topo_knn: k-nearest neighbor preservation\")\n",
    "print(\"  - topo_soft: Soft neighborhood distribution preservation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modified training script that supports VSP variants\n",
    "\n",
    "training_script = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "import toml\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "sys.path.insert(0, '/content/vec2vec')\n",
    "\n",
    "from utils.utils import load_n_translator\n",
    "from utils.model_utils import load_encoder, get_sentence_embedding_dimension\n",
    "from utils.streaming_utils import load_streaming_embeddings, MultiencoderTokenizedDataset, process_batch\n",
    "from utils.collate import TokenizedCollator\n",
    "from utils.train_utils import rec_loss_fn\n",
    "from utils.vsp_variants import get_vsp_loss_fn\n",
    "from utils.gan import LeastSquaresGAN\n",
    "from translators.Discriminator import Discriminator\n",
    "\n",
    "\n",
    "def create_config(vsp_type='original', num_points=50000, batch_size=64, epochs=3, lr=2e-5):\n",
    "    return {\n",
    "        'seed': 42,\n",
    "        'dataset': 'nq',\n",
    "        'unsup_emb': 'stella',\n",
    "        'sup_emb': 'gte',\n",
    "        'num_points': num_points,\n",
    "        'val_size': 4096,\n",
    "        'normalize_embeddings': True,\n",
    "        'mixed_precision': 'fp16',\n",
    "        'style': 'res_mlp',\n",
    "        'depth': 2,\n",
    "        'transform_depth': 3,\n",
    "        'd_adapter': 512,\n",
    "        'd_hidden': 512,\n",
    "        'norm_style': 'batch',\n",
    "        'gan_style': 'least_squares',\n",
    "        'disc_depth': 3,\n",
    "        'disc_dim': 256,\n",
    "        'bs': batch_size,\n",
    "        'lr': lr,\n",
    "        'disc_lr': lr,\n",
    "        'epochs': epochs,\n",
    "        'gradient_accumulation_steps': 2,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'loss_coefficient_rec': 1.0,\n",
    "        'loss_coefficient_vsp': 1.0,\n",
    "        'loss_coefficient_gen': 0.5,\n",
    "        'vsp_type': vsp_type,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_vec2vec(config, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config['mixed_precision'],\n",
    "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
    "    )\n",
    "\n",
    "    device = accelerator.device\n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"VSP type: {config['vsp_type']}\")\n",
    "\n",
    "    print(\"\\\\nLoading embedding models...\")\n",
    "    encoders = {}\n",
    "    encoders[config['unsup_emb']] = load_encoder(config['unsup_emb'])\n",
    "    encoders[config['sup_emb']] = load_encoder(config['sup_emb'])\n",
    "\n",
    "    for name, encoder in encoders.items():\n",
    "        encoder.eval()\n",
    "        for param in encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    unsup_dim = get_sentence_embedding_dimension(config['unsup_emb'])\n",
    "    sup_dim = get_sentence_embedding_dimension(config['sup_emb'])\n",
    "\n",
    "    print(\"\\\\nCreating translator...\")\n",
    "    from translators.TransformTranslator import TransformTranslator\n",
    "\n",
    "    translator = TransformTranslator(\n",
    "        dims={config['unsup_emb']: unsup_dim, config['sup_emb']: sup_dim},\n",
    "        adapter_depth=config['depth'],\n",
    "        d_adapter=config['d_adapter'],\n",
    "        d_hidden=config['d_hidden'],\n",
    "        n_style=config['style'],\n",
    "        transform_depth=config['transform_depth'],\n",
    "        norm_style=config['norm_style'],\n",
    "    )\n",
    "    translator = translator.to(device)\n",
    "\n",
    "    discriminator = Discriminator(\n",
    "        dims={config['unsup_emb']: unsup_dim, config['sup_emb']: sup_dim},\n",
    "        depth=config['disc_depth'],\n",
    "        hidden_dim=config['disc_dim'],\n",
    "    )\n",
    "    discriminator = discriminator.to(device)\n",
    "\n",
    "    gan = LeastSquaresGAN()\n",
    "\n",
    "    optimizer_g = torch.optim.AdamW(translator.parameters(), lr=config['lr'])\n",
    "    optimizer_d = torch.optim.AdamW(discriminator.parameters(), lr=config['disc_lr'])\n",
    "\n",
    "    vsp_loss_fn = get_vsp_loss_fn(config['vsp_type'])\n",
    "\n",
    "    print(\"\\\\nLoading data...\")\n",
    "    dset = load_streaming_embeddings(config['dataset'])\n",
    "    dset_dict = dset.train_test_split(test_size=config['val_size'], seed=42)\n",
    "    train_dset = dset_dict[\"train\"]\n",
    "\n",
    "    train_dset = train_dset.select(range(config['num_points']))\n",
    "\n",
    "    train_tokenized = MultiencoderTokenizedDataset(\n",
    "        train_dset, encoders,\n",
    "        n_embs_per_batch=1,\n",
    "        batch_size=config['bs'],\n",
    "        seed=config['seed']\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_tokenized,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        collate_fn=TokenizedCollator(),\n",
    "    )\n",
    "\n",
    "    translator, discriminator, optimizer_g, optimizer_d, train_loader = accelerator.prepare(\n",
    "        translator, discriminator, optimizer_g, optimizer_d, train_loader\n",
    "    )\n",
    "\n",
    "    print(f\"\\\\nStarting training for {config['epochs']} epochs...\")\n",
    "    print(f\"Training samples: {config['num_points']}\")\n",
    "    print(f\"Batch size: {config['bs']}\")\n",
    "\n",
    "    history = {'loss': [], 'rec_loss': [], 'vsp_loss': [], 'gen_loss': [], 'disc_loss': []}\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        translator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        epoch_losses = {'loss': 0, 'rec_loss': 0, 'vsp_loss': 0, 'gen_loss': 0, 'disc_loss': 0}\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        for batch in pbar:\n",
    "            ins = process_batch(batch, encoders, config['normalize_embeddings'], device)\n",
    "            recs, translations = translator(ins)\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "            disc_real = discriminator(ins)\n",
    "            disc_fake = discriminator({k: v[config['unsup_emb']] for k, v in translations.items()})\n",
    "            disc_loss = gan.discriminator_loss(disc_real, disc_fake)\n",
    "            accelerator.backward(disc_loss)\n",
    "            optimizer_d.step()\n",
    "\n",
    "            optimizer_g.zero_grad()\n",
    "            rec_loss = rec_loss_fn(ins, recs, None)\n",
    "            vsp_loss = vsp_loss_fn(ins, translations, None)\n",
    "            disc_fake_for_gen = discriminator({k: v[config['unsup_emb']] for k, v in translations.items()})\n",
    "            gen_loss = gan.generator_loss(disc_fake_for_gen)\n",
    "\n",
    "            total_loss = (\n",
    "                config['loss_coefficient_rec'] * rec_loss +\n",
    "                config['loss_coefficient_vsp'] * vsp_loss +\n",
    "                config['loss_coefficient_gen'] * gen_loss\n",
    "            )\n",
    "\n",
    "            accelerator.backward(total_loss)\n",
    "            torch.nn.utils.clip_grad_norm_(translator.parameters(), config['max_grad_norm'])\n",
    "            optimizer_g.step()\n",
    "\n",
    "            epoch_losses['loss'] += total_loss.item()\n",
    "            epoch_losses['rec_loss'] += rec_loss.item()\n",
    "            epoch_losses['vsp_loss'] += vsp_loss.item() if isinstance(vsp_loss, torch.Tensor) else vsp_loss\n",
    "            epoch_losses['gen_loss'] += gen_loss.item()\n",
    "            epoch_losses['disc_loss'] += disc_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{total_loss.item():.4f}\",\n",
    "                'rec': f\"{rec_loss.item():.4f}\",\n",
    "                'vsp': f\"{vsp_loss.item() if isinstance(vsp_loss, torch.Tensor) else vsp_loss:.4f}\",\n",
    "            })\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= num_batches\n",
    "            history[key].append(epoch_losses[key])\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {epoch_losses['loss']:.4f}, \"\n",
    "              f\"Rec: {epoch_losses['rec_loss']:.4f}, \"\n",
    "              f\"VSP: {epoch_losses['vsp_loss']:.4f}\")\n",
    "\n",
    "    print(f\"\\\\nSaving model to {output_dir}\")\n",
    "    torch.save({\n",
    "        'translator_state_dict': accelerator.unwrap_model(translator).state_dict(),\n",
    "        'discriminator_state_dict': accelerator.unwrap_model(discriminator).state_dict(),\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "    }, os.path.join(output_dir, 'checkpoint.pt'))\n",
    "\n",
    "    with open(os.path.join(output_dir, 'history.json'), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return history\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--vsp_type', type=str, default='original',\n",
    "                        choices=['original', 'conformal', 'topo_knn', 'topo_soft'])\n",
    "    parser.add_argument('--num_points', type=int, default=50000)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--epochs', type=int, default=3)\n",
    "    parser.add_argument('--lr', type=float, default=2e-5)\n",
    "    parser.add_argument('--output_dir', type=str, required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = create_config(\n",
    "        vsp_type=args.vsp_type,\n",
    "        num_points=args.num_points,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "        lr=args.lr,\n",
    "    )\n",
    "\n",
    "    train_vec2vec(config, args.output_dir)\n",
    "\"\"\"\n",
    "\n",
    "with open('/content/vec2vec/train_vsp_variants.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(\"✓ Created train_vsp_variants.py - modified training script with VSP variant support\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db64c0",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "### Hyperparameters for Colab (reduced for single GPU)\n",
    "\n",
    "| Parameter | Value | Notes |\n",
    "|-----------|-------|-------|\n",
    "| Training samples | 50,000 | Subset of full dataset |\n",
    "| Validation samples | 4,096 | For evaluation |\n",
    "| Batch size | 64 | Fits in T4/L4 GPU memory |\n",
    "| Epochs | 3 | Quick iteration; increase for better results |\n",
    "| Learning rate | 2e-5 | Default from paper |\n",
    "| Model pair | stella → gte | 768-dim to 768-dim |\n",
    "| Architecture | res_mlp | Residual MLP transform |\n",
    "| Mixed precision | fp16 | Memory efficiency |\n",
    "\n",
    "### Training Runs\n",
    "\n",
    "We'll train three models:\n",
    "1. **Original VSP** → `outputs/vec2vec_original/`\n",
    "2. **Conformal VSP** → `outputs/vec2vec_conformal/`\n",
    "3. **Topological kNN VSP** → `outputs/vec2vec_topo_knn/`\n",
    "\n",
    "Each run takes approximately 20-40 minutes on a T4 GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "import json\n",
    "\n",
    "# Common parameters\n",
    "COMMON_CONFIG = {\n",
    "    'num_points': 50000,   # Training samples (increase to 100k+ for better results)\n",
    "    'batch_size': 64,      # Batch size (reduce if OOM)\n",
    "    'epochs': 3,           # Training epochs (increase to 10+ for better results)\n",
    "    'lr': 2e-5,            # Learning rate\n",
    "}\n",
    "\n",
    "# VSP variants to train\n",
    "VSP_VARIANTS = ['original', 'conformal', 'topo_knn']\n",
    "\n",
    "# Display configurations\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Common parameters: {json.dumps(COMMON_CONFIG, indent=2)}\")\n",
    "print(f\"\\nVSP variants to train: {VSP_VARIANTS}\")\n",
    "print(\"\\nOutput directories:\")\n",
    "for vsp_type in VSP_VARIANTS:\n",
    "    print(f\"  - {vsp_type}: outputs/vec2vec_{vsp_type}/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"To adjust training:\")\n",
    "print(\"  - Increase num_points for more data\")\n",
    "print(\"  - Increase epochs for better convergence\")\n",
    "print(\"  - Reduce batch_size if out of memory\")\n",
    "print(\"  - Comment out VSP variants in VSP_VARIANTS to skip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517589ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all VSP variants\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Training parameters (can modify above)\n",
    "num_points = COMMON_CONFIG['num_points']\n",
    "batch_size = COMMON_CONFIG['batch_size']\n",
    "epochs = COMMON_CONFIG['epochs']\n",
    "lr = COMMON_CONFIG['lr']\n",
    "\n",
    "# Train each variant\n",
    "results = {}\n",
    "\n",
    "for vsp_type in VSP_VARIANTS:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Training vec2vec with VSP type: {vsp_type}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    output_dir = f\"/content/vec2vec/outputs/vec2vec_{vsp_type}\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run training\n",
    "    cmd = [\n",
    "        \"python\", \"/content/vec2vec/train_vsp_variants.py\",\n",
    "        \"--vsp_type\", vsp_type,\n",
    "        \"--num_points\", str(num_points),\n",
    "        \"--batch_size\", str(batch_size),\n",
    "        \"--epochs\", str(epochs),\n",
    "        \"--lr\", str(lr),\n",
    "        \"--output_dir\", output_dir,\n",
    "    ]\n",
    "\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, cwd=\"/content/vec2vec\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    results[vsp_type] = {\n",
    "        'output_dir': output_dir,\n",
    "        'elapsed_time': elapsed,\n",
    "        'success': result.returncode == 0\n",
    "    }\n",
    "\n",
    "    print(f\"\\nCompleted {vsp_type} in {elapsed/60:.1f} minutes\")\n",
    "    print(f\"Output saved to: {output_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for vsp_type, info in results.items():\n",
    "    status = \"✓\" if info['success'] else \"✗\"\n",
    "    print(f\"{status} {vsp_type}: {info['elapsed_time']/60:.1f} min - {info['output_dir']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc441f71",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate each trained model on a held-out test set using:\n",
    "\n",
    "### Metrics\n",
    "\n",
    "1. **Cosine Similarity**\n",
    "   - Average cosine similarity between translated embeddings and ground-truth target embeddings\n",
    "   - Range: [-1, 1], higher is better\n",
    "   - Measures direct alignment quality\n",
    "\n",
    "2. **Top-1 Accuracy**\n",
    "   - For each translated embedding, find the nearest neighbor in the target embedding space\n",
    "   - Check if the nearest neighbor is the correct corresponding embedding\n",
    "   - Range: [0, 1], higher is better\n",
    "   - Measures retrieval accuracy\n",
    "\n",
    "3. **Mean Rank**\n",
    "   - For each translated embedding, rank all target embeddings by similarity\n",
    "   - Report the average rank of the correct target\n",
    "   - Lower is better (1 = perfect)\n",
    "   - Measures how well the translation preserves relative positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ae23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation script\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/vec2vec')\n",
    "\n",
    "from utils.model_utils import load_encoder, get_sentence_embedding_dimension\n",
    "from utils.streaming_utils import load_streaming_embeddings, MultiencoderTokenizedDataset, process_batch\n",
    "from utils.collate import TokenizedCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from translators.TransformTranslator import TransformTranslator\n",
    "\n",
    "\n",
    "def evaluate_model(checkpoint_path, test_size=8192, batch_size=128):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config = checkpoint['config']\n",
    "\n",
    "    print(f\"\\nEvaluating model: {checkpoint_path}\")\n",
    "    print(f\"VSP type: {config['vsp_type']}\")\n",
    "\n",
    "    encoders = {}\n",
    "    encoders[config['unsup_emb']] = load_encoder(config['unsup_emb'])\n",
    "    encoders[config['sup_emb']] = load_encoder(config['sup_emb'])\n",
    "\n",
    "    for encoder in encoders.values():\n",
    "        encoder.eval()\n",
    "        for param in encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    unsup_dim = get_sentence_embedding_dimension(config['unsup_emb'])\n",
    "    sup_dim = get_sentence_embedding_dimension(config['sup_emb'])\n",
    "\n",
    "    translator = TransformTranslator(\n",
    "        dims={config['unsup_emb']: unsup_dim, config['sup_emb']: sup_dim},\n",
    "        adapter_depth=config['depth'],\n",
    "        d_adapter=config['d_adapter'],\n",
    "        d_hidden=config['d_hidden'],\n",
    "        n_style=config['style'],\n",
    "        transform_depth=config['transform_depth'],\n",
    "        norm_style=config['norm_style'],\n",
    "    )\n",
    "    translator.load_state_dict(checkpoint['translator_state_dict'])\n",
    "    translator = translator.to(device)\n",
    "    translator.eval()\n",
    "\n",
    "    dset = load_streaming_embeddings(config['dataset'])\n",
    "    dset_dict = dset.train_test_split(test_size=test_size + config['val_size'], seed=42)\n",
    "    test_dset = dset_dict[\"test\"].select(range(test_size))\n",
    "\n",
    "    test_tokenized = MultiencoderTokenizedDataset(\n",
    "        test_dset, encoders,\n",
    "        n_embs_per_batch=1,\n",
    "        batch_size=batch_size,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_tokenized,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        collate_fn=TokenizedCollator(),\n",
    "    )\n",
    "\n",
    "    all_source = []\n",
    "    all_target = []\n",
    "    all_translated = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Processing test data\"):\n",
    "            ins = process_batch(batch, encoders, config['normalize_embeddings'], device)\n",
    "\n",
    "            source_emb = ins[config['unsup_emb']]\n",
    "            target_emb = ins[config['sup_emb']]\n",
    "\n",
    "            _, translations = translator(ins)\n",
    "            translated_emb = translations[config['sup_emb']][config['unsup_emb']]\n",
    "\n",
    "            all_source.append(source_emb.cpu())\n",
    "            all_target.append(target_emb.cpu())\n",
    "            all_translated.append(translated_emb.cpu())\n",
    "\n",
    "    all_source = torch.cat(all_source, dim=0)\n",
    "    all_target = torch.cat(all_target, dim=0)\n",
    "    all_translated = torch.cat(all_translated, dim=0)\n",
    "\n",
    "    print(f\"Test samples: {all_source.shape[0]}\")\n",
    "\n",
    "    # 1. Cosine similarity\n",
    "    cosine_sims = F.cosine_similarity(all_translated, all_target, dim=1)\n",
    "    mean_cosine = cosine_sims.mean().item()\n",
    "\n",
    "    # 2. Top-1 accuracy and rank\n",
    "    translated_norm = F.normalize(all_translated, dim=1)\n",
    "    target_norm = F.normalize(all_target, dim=1)\n",
    "\n",
    "    similarity_matrix = translated_norm @ target_norm.T\n",
    "\n",
    "    top1_predictions = similarity_matrix.argmax(dim=1)\n",
    "    correct_indices = torch.arange(all_translated.shape[0])\n",
    "    top1_accuracy = (top1_predictions == correct_indices).float().mean().item()\n",
    "\n",
    "    ranks = []\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        sims = similarity_matrix[i]\n",
    "        sorted_indices = sims.argsort(descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "        ranks.append(rank)\n",
    "    mean_rank = np.mean(ranks)\n",
    "\n",
    "    results = {\n",
    "        'vsp_type': config['vsp_type'],\n",
    "        'train_size': config['num_points'],\n",
    "        'test_size': all_source.shape[0],\n",
    "        'cosine_similarity': mean_cosine,\n",
    "        'top1_accuracy': top1_accuracy,\n",
    "        'mean_rank': mean_rank,\n",
    "    }\n",
    "\n",
    "    print(f\"Results for {config['vsp_type']}:\")\n",
    "    print(f\"  Cosine Similarity: {mean_cosine:.4f}\")\n",
    "    print(f\"  Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "    print(f\"  Mean Rank: {mean_rank:.2f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate all trained models\n",
    "all_results = []\n",
    "\n",
    "for vsp_type in VSP_VARIANTS:\n",
    "    checkpoint_path = f\"/content/vec2vec/outputs/vec2vec_{vsp_type}/checkpoint.pt\"\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            result = evaluate_model(checkpoint_path, test_size=8192, batch_size=128)\n",
    "            all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {vsp_type}: {e}\")\n",
    "    else:\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "# Create results table\n",
    "if all_results:\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df[['vsp_type', 'train_size', 'test_size', 'cosine_similarity', 'top1_accuracy', 'mean_rank']]\n",
    "    df.columns = ['VSP Type', 'Train Size', 'Test Size', 'Cosine Sim', 'Top-1 Acc', 'Mean Rank']\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    df.to_csv('/content/vec2vec/outputs/vsp_comparison_results.csv', index=False)\n",
    "    print(\"\\nResults saved to outputs/vsp_comparison_results.csv\")\n",
    "else:\n",
    "    print(\"No models were successfully evaluated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "if all_results:\n",
    "    df = pd.DataFrame(all_results)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    colors = sns.color_palette(\"husl\", len(df))\n",
    "\n",
    "    # 1. Cosine Similarity\n",
    "    ax = axes[0]\n",
    "    bars = ax.bar(df['vsp_type'], df['cosine_similarity'], color=colors, edgecolor='black')\n",
    "    ax.set_xlabel('VSP Type', fontsize=12)\n",
    "    ax.set_ylabel('Cosine Similarity', fontsize=12)\n",
    "    ax.set_title('Cosine Similarity (Higher is Better)', fontsize=14)\n",
    "    ax.set_ylim([0, 1])\n",
    "    for bar, val in zip(bars, df['cosine_similarity']):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # 2. Top-1 Accuracy\n",
    "    ax = axes[1]\n",
    "    bars = ax.bar(df['vsp_type'], df['top1_accuracy'], color=colors, edgecolor='black')\n",
    "    ax.set_xlabel('VSP Type', fontsize=12)\n",
    "    ax.set_ylabel('Top-1 Accuracy', fontsize=12)\n",
    "    ax.set_title('Top-1 Accuracy (Higher is Better)', fontsize=14)\n",
    "    ax.set_ylim([0, 1])\n",
    "    for bar, val in zip(bars, df['top1_accuracy']):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # 3. Mean Rank\n",
    "    ax = axes[2]\n",
    "    bars = ax.bar(df['vsp_type'], df['mean_rank'], color=colors, edgecolor='black')\n",
    "    ax.set_xlabel('VSP Type', fontsize=12)\n",
    "    ax.set_ylabel('Mean Rank', fontsize=12)\n",
    "    ax.set_title('Mean Rank (Lower is Better)', fontsize=14)\n",
    "    for bar, val in zip(bars, df['mean_rank']):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/vec2vec/outputs/vsp_comparison_plot.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nPlot saved to outputs/vsp_comparison_plot.png\")\n",
    "\n",
    "    # Plot training curves if available\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    for vsp_type in VSP_VARIANTS:\n",
    "        history_path = f\"/content/vec2vec/outputs/vec2vec_{vsp_type}/history.json\"\n",
    "        if os.path.exists(history_path):\n",
    "            with open(history_path) as f:\n",
    "                history = json.load(f)\n",
    "\n",
    "            epochs = range(1, len(history['loss']) + 1)\n",
    "\n",
    "            axes2[0].plot(epochs, history['loss'], marker='o', label=vsp_type)\n",
    "            axes2[1].plot(epochs, history['vsp_loss'], marker='o', label=vsp_type)\n",
    "\n",
    "    axes2[0].set_xlabel('Epoch')\n",
    "    axes2[0].set_ylabel('Total Loss')\n",
    "    axes2[0].set_title('Training Loss')\n",
    "    axes2[0].legend()\n",
    "\n",
    "    axes2[1].set_xlabel('Epoch')\n",
    "    axes2[1].set_ylabel('VSP Loss')\n",
    "    axes2[1].set_title('VSP Loss')\n",
    "    axes2[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/vec2vec/outputs/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training curves saved to outputs/training_curves.png\")\n",
    "else:\n",
    "    print(\"No results to visualize.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1cbe2",
   "metadata": {},
   "source": [
    "## Summary & Discussion\n",
    "\n",
    "### What This Notebook Reproduces\n",
    "\n",
    "This is a **scaled-down reproduction** of vec2vec with VSP variants:\n",
    "- **Model pair**: stella → gte (768-dim text embeddings)\n",
    "- **Dataset**: Natural Questions (NQ)\n",
    "- **Training size**: ~50,000 examples (full paper uses 100k+)\n",
    "- **Epochs**: 3 (increase for better results)\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "Compare the three VSP variants:\n",
    "\n",
    "1. **Original VSP** (baseline)\n",
    "   - Preserves raw pairwise similarities\n",
    "   - What the paper uses\n",
    "\n",
    "2. **Conformal VSP**\n",
    "   - Preserves angles/cosine similarities more explicitly\n",
    "   - May help if the embedding spaces have different scales\n",
    "\n",
    "3. **Topological kNN VSP**\n",
    "   - Preserves neighborhood structure\n",
    "   - More flexible - does not require exact similarity preservation\n",
    "   - May help with noisy or sparse embeddings\n",
    "\n",
    "### Key Questions to Answer\n",
    "\n",
    "- **Does conformal VSP improve upon original?**\n",
    "  - If yes: angle preservation matters more than raw dot products\n",
    "\n",
    "- **Does topological VSP improve upon original?**\n",
    "  - If yes: neighborhood structure is more important than exact similarities\n",
    "\n",
    "- **Trade-offs between metrics?**\n",
    "  - A method might improve Top-1 but hurt Mean Rank, or vice versa\n",
    "\n",
    "### How to Improve Results\n",
    "\n",
    "1. **Increase training data**: `num_points = 100000` or more\n",
    "2. **More epochs**: `epochs = 10` or more\n",
    "3. **Tune hyperparameters**: Learning rate, batch size, model capacity\n",
    "4. **Try other model pairs**: gte→gtr, e5→gtr, etc.\n",
    "5. **Adjust kNN k**: Try k=3, k=10 for topological VSP\n",
    "6. **Adjust temperature**: For topo_soft variant\n",
    "\n",
    "### Code Locations\n",
    "\n",
    "| Component | Location |\n",
    "|-----------|----------|\n",
    "| VSP variants | `utils/vsp_variants.py` |\n",
    "| Training script | `train_vsp_variants.py` |\n",
    "| Results | `outputs/vsp_comparison_results.csv` |\n",
    "| Plots | `outputs/vsp_comparison_plot.png` |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run with more data and epochs\n",
    "2. Try additional VSP variants (topo_soft)\n",
    "3. Experiment with different model pairs\n",
    "4. Compare with optimal transport baseline\n",
    "5. Analyze per-sample results for failure cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674292c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - utils/vsp_variants.py (VSP loss implementations)\")\n",
    "print(\"  - train_vsp_variants.py (training script)\")\n",
    "print(\"  - outputs/vec2vec_*/checkpoint.pt (trained models)\")\n",
    "print(\"  - outputs/vsp_comparison_results.csv (evaluation results)\")\n",
    "print(\"  - outputs/*.png (visualization plots)\")\n",
    "\n",
    "print(\"\\nTo re-run with different settings:\")\n",
    "print(\"  1. Modify COMMON_CONFIG in cell 11\")\n",
    "print(\"  2. Modify VSP_VARIANTS to add/remove variants\")\n",
    "print(\"  3. Re-run cells 12-15\")\n",
    "\n",
    "print(\"\\nTo use a trained model:\")\n",
    "print(\"  checkpoint = torch.load('outputs/vec2vec_original/checkpoint.pt')\")\n",
    "print(\"  translator.load_state_dict(checkpoint['translator_state_dict'])\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
